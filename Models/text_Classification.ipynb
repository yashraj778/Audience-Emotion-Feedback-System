{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iazwzcWxEeE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a909645-fb50-4ff4-84b3-97fc58772634"
      },
      "source": [
        "# mounting google drive for reading files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeowYZW4NGSC",
        "outputId": "15165f0b-788c-471e-d8d8-1a0f428b4b04"
      },
      "source": [
        "# loading the required packages\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPb7Pw6MNKH_"
      },
      "source": [
        "# Loading Data and creating Labels from data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syXWaseEEoU0"
      },
      "source": [
        "path = '/content/drive/MyDrive/EMOJI Classification/full_set.txt'\n",
        "with open(path) as f:\n",
        "  content = f.readlines()\n",
        "content = [x.strip() for x in content]\n",
        "sentences = [x.split(\"\\t\")[0] for x in content]\n",
        "labels = [x.split(\"\\t\")[1] for x in content]\n",
        "y = np.array(labels,dtype='int8')\n",
        "y = 2*y - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N4PHSYKP7fA"
      },
      "source": [
        "# Preprocessing text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qasMSVmYFYTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e400e211-caa4-4ee3-8259-26aca3628edd"
      },
      "source": [
        "# Adding Stemming Technique\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#porter=nltk.PorterStemmer()\n",
        "porter=WordNetLemmatizer()\n",
        "sentence_filtered=[]\n",
        "for x in sentences:\n",
        "  x=x.lower()\n",
        "  x= re.sub(r'[^\\w]',' ',x)\n",
        "  x = re.sub(r'[0-9]+','',x)\n",
        "  #sentence_filtered.append(' '.join([porter.lemmatize(word) for word in x.split() if word not in stop_words]))\n",
        "  sentence_filtered.append(' '.join([porter.lemmatize(word) for word in x.split()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-ys-CrqV7SJ"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9aZqvJOWZp3"
      },
      "source": [
        "# Applying TF IDF Vector\n",
        "vectorizer = CountVectorizer(analyzer='word',preprocessor=None,max_features=6000,ngram_range=(1,3))\n",
        "data_features = vectorizer.fit_transform(sentence_filtered)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "data_mat = tfidf_transformer.fit_transform(data_features).toarray()\n",
        "#Splitting data into Train and Test\n",
        "np.random.seed(0)\n",
        "test_index = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
        "train_index = list(set(range(len(labels))) - set(test_index))\n",
        "train_data = data_mat[train_index,]\n",
        "train_labels = y[train_index]\n",
        "test_data = data_mat[test_index,]\n",
        "test_labels = y[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6mC8sLi_VVa"
      },
      "source": [
        "# Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlFlF3oAOyLQ",
        "outputId": "0b756228-6642-49b6-b7ca-709c7719ee04"
      },
      "source": [
        "#Applying Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(random_state=0).fit(train_data, train_labels)\n",
        "preds_train = clf.predict(train_data)\n",
        "preds_test = clf.predict(test_data)\n",
        "## Compute errors\n",
        "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
        "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
        "print(\"Training error: \", float(errs_train)/len(train_labels))\n",
        "print(\"Test error: \", float(errs_test)/len(test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training error:  0.0492\n",
            "Test error:  0.166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tSzozYjPH0e",
        "outputId": "8194b8e7-d323-4d56-d25d-e64b0b677534"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_labels,preds_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.81      0.87      0.84       250\n",
            "           1       0.86      0.80      0.83       250\n",
            "\n",
            "    accuracy                           0.83       500\n",
            "   macro avg       0.84      0.83      0.83       500\n",
            "weighted avg       0.84      0.83      0.83       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CjPg7mC_cEt"
      },
      "source": [
        "# SGD Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-BMBYbJXAc6",
        "outputId": "7e1d6cb3-2deb-4726-dc9e-c843ada0d068"
      },
      "source": [
        "# Applying SGD classifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "clf = SGDClassifier(loss=\"log\", penalty=\"none\")\n",
        "clf.fit(train_data, train_labels)\n",
        "preds_train = clf.predict(train_data)\n",
        "preds_test = clf.predict(test_data)\n",
        "## Compute errors\n",
        "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
        "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
        "print(\"Training error: \", float(errs_train)/len(train_labels))\n",
        "print(\"Test error: \", float(errs_test)/len(test_labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training error:  0.002\n",
            "Test error:  0.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14tlL-0AvQ0X"
      },
      "source": [
        "# saving the vectorizer \n",
        "import pickle\n",
        "filename = '/content/drive/MyDrive/EMOJI Classification/vector_data.pkl'\n",
        "pickle.dump(vectorizer, open(filename, 'wb'))\n",
        "# loading the vectorizer\n",
        "vec = pickle.load(open( filename, 'rb' ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXXdgKYKPZ0s"
      },
      "source": [
        "# saving the SGD classifier model \n",
        "import pickle\n",
        "filename = '/content/drive/MyDrive/EMOJI Classification/finalized_model.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNv4f7SAsyKo",
        "outputId": "a7caa272-d9a6-4c29-b2b0-95b1e55bfabb"
      },
      "source": [
        "review = 'so bad'\n",
        "inp = vec.transform([review])\n",
        "# loading the model\n",
        "loaded_model.predict(inp)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1], dtype=int8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5CHmC26wpgh",
        "outputId": "a152a75b-0ac9-447c-8a2f-9f1f597c23fd"
      },
      "source": [
        "# predicting the probability \n",
        "loaded_model.predict_proba(test_data[0:1,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99654479, 0.00345521]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wZJUl_hdeHU",
        "outputId": "03c24ada-5508-487a-bb83-43b771b5bec3"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_labels,preds_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.85      0.84      0.85       250\n",
            "           1       0.85      0.86      0.85       250\n",
            "\n",
            "    accuracy                           0.85       500\n",
            "   macro avg       0.85      0.85      0.85       500\n",
            "weighted avg       0.85      0.85      0.85       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ljyef3r_h04"
      },
      "source": [
        "# Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXcLv9Kn3uGz",
        "outputId": "040cb064-0746-47c1-ac08-d2092cfc6a89"
      },
      "source": [
        "# Applying naive bayes method\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb_clf = MultinomialNB().fit(train_data, train_labels)\n",
        "nb_preds_test = nb_clf.predict(test_data)\n",
        "nb_errs_test = np.sum((nb_preds_test > 0.0) != (test_labels > 0.0))\n",
        "print(\"Test error: \", float(nb_errs_test)/len(test_labels))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test error:  0.158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9zWPBYyPM8i",
        "outputId": "a2b96fbd-0050-4703-9421-1e3fa846bd89"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_labels,preds_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.85      0.84      0.85       250\n",
            "           1       0.85      0.86      0.85       250\n",
            "\n",
            "    accuracy                           0.85       500\n",
            "   macro avg       0.85      0.85      0.85       500\n",
            "weighted avg       0.85      0.85      0.85       500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJCOcebn36aP",
        "outputId": "df2fd035-27a5-4eab-867f-9950c520e9e3"
      },
      "source": [
        "print(nb_clf.predict(vectorizer.transform([\"event is awesome\"])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXMudZfy_lWX"
      },
      "source": [
        "# SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoy0ctAwBlDR",
        "outputId": "61d0de26-967a-412b-9c59-662d094c5cc9"
      },
      "source": [
        "# building the model using SVM\n",
        "from sklearn.svm import SVC\n",
        "svm = SVC()\n",
        "svm.fit(train_data, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfGS8iF0B2jI",
        "outputId": "395bb7ee-202c-4340-f291-8081d157f0b5"
      },
      "source": [
        "svm_preds_test = svm.predict(test_data)\n",
        "svm_errs_test = np.sum((svm_preds_test > 0.0) != (test_labels > 0.0))\n",
        "print(\"Test error: \", float(svm_errs_test)/len(test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test error:  0.154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWEvm-fUyu1t",
        "outputId": "1f8e0e30-9120-4fa5-b3c1-c82a48487625"
      },
      "source": [
        "svm.predict(vectorizer.transform([\"it is not good\"]).reshape(1,-1).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1], dtype=int8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwpEz9zI_oJZ"
      },
      "source": [
        "# LSTM Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfRkrNF74sEB",
        "outputId": "52ba15f5-8b3d-4edf-b1ec-62009fcf704a"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "max_review_length = 200\n",
        "tokenizer = Tokenizer(num_words=10000,  #max no. of unique words to keep\n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
        "                      lower=True #convert to lower case\n",
        "                     )\n",
        "#tokenizer.fit_on_texts(sentence_filtered)\n",
        "X = tokenizer.texts_to_sequences(sentence_filtered)\n",
        "X = sequence.pad_sequences(X, maxlen= max_review_length)\n",
        "print('Shape of data tensor:', X.shape)\n",
        "import pandas as pd\n",
        "Y=pd.get_dummies(y).values\n",
        "np.random.seed(0)\n",
        "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
        "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
        "train_data = X[train_inds,]\n",
        "train_labels = Y[train_inds]\n",
        "test_data = X[test_inds,]\n",
        "test_labels = Y[test_inds]\n",
        "EMBEDDING_DIM = 64\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "#model.add(SpatialDropout1D(0.2))\n",
        "# model.add(LSTM(250, dropout=0.2,return_sequences=True))\n",
        "model.add(LSTM(100, dropout=0.2, return_sequences=True))\n",
        "model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (3000, 200)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 64)           640000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 200, 100)          66000     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 50)                30200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 102       \n",
            "=================================================================\n",
            "Total params: 736,302\n",
            "Trainable params: 736,302\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMrKhL9Z5iOc",
        "outputId": "7bbb3d1f-d8a2-4fe6-b064-7a1e116d7251"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 50\n",
        "model.fit(train_data, train_labels, \n",
        "          epochs=epochs, \n",
        "          batch_size=batch_size,\n",
        "          validation_split=0.1)\n",
        "loss, acc = model.evaluate(test_data, test_labels, verbose=2,\n",
        "                            batch_size=batch_size)\n",
        "print(f\"loss: {loss}\")\n",
        "print(f\"Validation accuracy: {acc}\")\n",
        "outcome_labels = ['Negative', 'Positive']\n",
        "new = [\"I would not recommend this movie\"]\n",
        "    \n",
        "seq = tokenizer.texts_to_sequences(new)\n",
        "padded = sequence.pad_sequences(seq, maxlen=max_review_length)\n",
        "pred = model.predict(padded)\n",
        "print(\"Probability distribution: \", pred)\n",
        "print(\"Is this a Positive or Negative review? \")\n",
        "print(outcome_labels[np.argmax(pred)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "45/45 [==============================] - 26s 483ms/step - loss: 0.6944 - accuracy: 0.5094 - val_loss: 0.6986 - val_accuracy: 0.3960\n",
            "Epoch 2/10\n",
            "45/45 [==============================] - 21s 463ms/step - loss: 0.6930 - accuracy: 0.5153 - val_loss: 0.6982 - val_accuracy: 0.3960\n",
            "Epoch 3/10\n",
            "45/45 [==============================] - 21s 460ms/step - loss: 0.6937 - accuracy: 0.5023 - val_loss: 0.6956 - val_accuracy: 0.3960\n",
            "Epoch 4/10\n",
            "45/45 [==============================] - 21s 463ms/step - loss: 0.6930 - accuracy: 0.5164 - val_loss: 0.6984 - val_accuracy: 0.3960\n",
            "Epoch 5/10\n",
            "45/45 [==============================] - 21s 465ms/step - loss: 0.6925 - accuracy: 0.5175 - val_loss: 0.6951 - val_accuracy: 0.3960\n",
            "Epoch 6/10\n",
            "45/45 [==============================] - 21s 462ms/step - loss: 0.6937 - accuracy: 0.4738 - val_loss: 0.7010 - val_accuracy: 0.3960\n",
            "Epoch 7/10\n",
            "45/45 [==============================] - 21s 468ms/step - loss: 0.6930 - accuracy: 0.5211 - val_loss: 0.6972 - val_accuracy: 0.3960\n",
            "Epoch 8/10\n",
            "45/45 [==============================] - 21s 463ms/step - loss: 0.6928 - accuracy: 0.5237 - val_loss: 0.6965 - val_accuracy: 0.3960\n",
            "Epoch 9/10\n",
            "45/45 [==============================] - 21s 463ms/step - loss: 0.6924 - accuracy: 0.5291 - val_loss: 0.6986 - val_accuracy: 0.3960\n",
            "Epoch 10/10\n",
            "45/45 [==============================] - 21s 472ms/step - loss: 0.6942 - accuracy: 0.4626 - val_loss: 0.6997 - val_accuracy: 0.3960\n",
            "10/10 - 1s - loss: 0.6936 - accuracy: 0.5000\n",
            "loss: 0.6935827732086182\n",
            "Validation accuracy: 0.5\n",
            "Probability distribution:  [[0.5147547 0.4852453]]\n",
            "Is this a Positive or Negative review? \n",
            "Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPrPMVEIAOjj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}